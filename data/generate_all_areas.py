"""Generate 6 skill-area content for ALL words using Gemini API."""
import json, time, requests, os

GEMINI_API_KEY = "AIzaSyAGf0KxjRX7aiqBJAv_E6hCoqU9fT2DVgg"
GEMINI_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key={GEMINI_API_KEY}"
BATCH_SIZE = 20
DELAY = 2
DATA_DIR = os.path.dirname(os.path.abspath(__file__))
PROGRESS_FILE = os.path.join(DATA_DIR, "progress.json")
WORDS_FILE = os.path.join(DATA_DIR, "all_words.json")

AREA_NAMES = {
    "1": "의미파악력",
    "2": "단어연상력",
    "3": "발음청취력",
    "4": "어휘추론력",
    "5": "철자기억력",
    "6": "종합응용력",
}

PROMPT_TEMPLATE = """You are an English vocabulary content generator for Korean students.
For each word below, generate content for 6 skill areas:

1. 의미파악력 (Meaning): One simple English sentence (under 12 words) that clearly shows the word's meaning.
2. 단어연상력 (Association): 3 related English words (synonyms, antonyms, or thematic).
3. 발음청취력 (Listening): One English word that sounds similar or is commonly confused.
4. 어휘추론력 (Inference): One English sentence with the target word replaced by "___" where context makes meaning guessable.
5. 철자기억력 (Spelling): The word with some letters replaced by underscores (keep first letter, remove ~40%%).
6. 종합응용력 (Application): One natural English sentence using the word in real-life context (under 15 words).

Return ONLY a JSON array. Each element: {"w":"word","1":"...","2":"...","3":"...","4":"...","5":"...","6":"..."}

Words:
"""


def generate_batch(words_batch, retries=3):
    word_list = "\n".join(
        f"- {w['english']} ({w['pos']}, {w['korean']})" for w in words_batch
    )
    body = {
        "contents": [{"parts": [{"text": PROMPT_TEMPLATE + word_list}]}],
        "generationConfig": {
            "temperature": 0.7,
            "maxOutputTokens": 4096,
            "responseMimeType": "application/json",
        },
    }

    for attempt in range(retries):
        try:
            resp = requests.post(GEMINI_URL, json=body, timeout=60)
            if resp.status_code == 429:
                wait = 10 * (attempt + 1)
                print(f"RATE_LIMIT({wait}s)", end=" ", flush=True)
                time.sleep(wait)
                continue
            resp.raise_for_status()
            data = resp.json()
            text_out = data["candidates"][0]["content"]["parts"][0]["text"]
            return json.loads(text_out)
        except (json.JSONDecodeError, KeyError, IndexError):
            if attempt < retries - 1:
                print("RETRY", end=" ", flush=True)
                time.sleep(3)
            else:
                return None
        except requests.exceptions.RequestException as e:
            if attempt < retries - 1:
                print("NET_RETRY", end=" ", flush=True)
                time.sleep(5)
            else:
                return None
    return None


def load_progress():
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"completed_batches": 0, "results": []}


def save_progress(progress):
    with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
        json.dump(progress, f, ensure_ascii=False)


def save_markdown_files(all_results, all_words):
    word_info = {}
    for w in all_words:
        word_info[w["english"].lower()] = w

    for area_key, area_name in AREA_NAMES.items():
        lines = [
            f"# {area_name} - 전체 단어",
            "",
            f"> Generated by Gemini 2.5 Flash-Lite | {len(all_results)} words",
            "",
        ]

        current_book = ""
        current_lesson = ""

        sorted_results = []
        for r in all_results:
            info = word_info.get(r.get("w", "").lower(), {})
            sorted_results.append({
                **r,
                "book": info.get("book_name", "Unknown"),
                "lesson": info.get("lesson", "Unknown"),
                "korean": info.get("korean", ""),
            })

        headers = {
            "2": ("Related Words",),
            "3": ("Confusing Word",),
            "5": ("Spelling Hint",),
        }

        for r in sorted_results:
            if r["book"] != current_book:
                current_book = r["book"]
                current_lesson = ""
                lines.append(f"## {current_book}")
                lines.append("")

            if r["lesson"] != current_lesson:
                current_lesson = r["lesson"]
                col = headers.get(area_key, ("Content",))[0]
                lines.append(f"### {current_lesson}")
                lines.append("")
                lines.append(f"| English | Korean | {col} |")
                lines.append("|---------|--------|" + "-" * (len(col) + 2) + "|")

            content = r.get(area_key, "")
            lines.append(f"| {r.get('w', '')} | {r['korean']} | {content} |")

        lines.append("")
        filepath = os.path.join(DATA_DIR, f"area{area_key}_{area_name}.md")
        with open(filepath, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        print(f"  Saved: area{area_key}_{area_name}.md")


def main():
    with open(WORDS_FILE, "r", encoding="utf-8") as f:
        words = json.load(f)
    print(f"Total words: {len(words)}")

    progress = load_progress()
    start_batch = progress["completed_batches"]
    all_results = progress["results"]
    total_batches = (len(words) + BATCH_SIZE - 1) // BATCH_SIZE

    if start_batch > 0:
        print(f"Resuming from batch {start_batch + 1}/{total_batches} ({len(all_results)} done)")

    start_time = time.time()
    failed = 0

    for i in range(start_batch * BATCH_SIZE, len(words), BATCH_SIZE):
        batch = words[i: i + BATCH_SIZE]
        batch_num = i // BATCH_SIZE + 1
        print(f"  [{batch_num}/{total_batches}] {batch[0]['book_name']}/{batch[0]['lesson']}...", end=" ", flush=True)

        results = generate_batch(batch)
        if results:
            result_map = {r.get("w", "").lower(): r for r in results}
            for w in batch:
                matched = result_map.get(w["english"].lower())
                if matched:
                    all_results.append(matched)
                else:
                    all_results.append({"w": w["english"]})
            print(f"OK ({len(results)})")
        else:
            failed += 1
            for w in batch:
                all_results.append({"w": w["english"]})
            print("FAILED")

        progress["completed_batches"] = batch_num
        progress["results"] = all_results
        if batch_num % 25 == 0:
            save_progress(progress)
            elapsed = time.time() - start_time
            done = batch_num - start_batch
            remaining = elapsed / done * (total_batches - batch_num)
            print(f"    === {batch_num}/{total_batches} | {elapsed:.0f}s elapsed | ~{remaining:.0f}s remaining ===")

        if i + BATCH_SIZE < len(words):
            time.sleep(DELAY)

    save_progress(progress)
    elapsed = time.time() - start_time
    success = sum(1 for r in all_results if r.get("1"))
    print(f"\nComplete! {success}/{len(all_results)} success | {failed} failed batches | {elapsed:.0f}s")

    print("\nSaving 6 markdown files...")
    save_markdown_files(all_results, words)
    print("Done!")


if __name__ == "__main__":
    main()
